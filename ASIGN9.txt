1.Hive is an open-source data warehousing system that is built on top of Hadoop. It provides a SQL-like interface to query and analyze large datasets stored in Hadoop's distributed file system (HDFS) or other compatible storage systems. The present version of Hive is 3.1.2, as of the knowledge cutoff date of September 2021.
2.No, Hive is not suitable for OLTP (online transaction processing) systems. Hive is designed for OLAP (online analytical processing) systems, which are optimized for complex queries over large datasets. OLTP systems are optimized for fast, transactional processing of small amounts of data.
3.Hive is different from RDBMS (relational database management systems) in several ways. RDBMS are designed for transactional processing and support ACID (atomicity, consistency, isolation, and durability) transactions, while Hive is designed for analytical processing and does not support ACID transactions. Hive is also optimized for handling large datasets that are stored in a distributed file system, while RDBMS are typically used for smaller datasets stored in a centralized database.
4.The Hive architecture consists of three main components: the HiveQL compiler, the Hive metastore, and the Hive execution engine. The HiveQL compiler translates HiveQL queries into MapReduce jobs or Tez DAGs (directed acyclic graphs). The Hive metastore stores metadata about tables and partitions, such as their schema and location. The Hive execution engine executes the MapReduce jobs or Tez DAGs generated by the HiveQL compiler.
5.The Hive query processor processes HiveQL queries by parsing them, optimizing them, and generating an execution plan for them. The components of a Hive query processor include the parser, the optimizer, and the execution engine
6.The three different modes in which we can operate Hive are:

Local mode: Hive runs on a single machine and uses a local file system for storage.
MapReduce mode: Hive runs on a Hadoop cluster and uses HDFS for storage.
Tez mode: Hive runs on a Hadoop cluster and uses the Tez framework for query execution.
7. Features of Hive include its SQL-like syntax, support for multiple storage systems, ability to handle large datasets, and integration with Hadoop. Limitations of Hive include its lack of support for ACID transactions, limited support for real-time processing, and relatively slow query performance compared to traditional databases.
8. To create a database in Hive, you can use the following command:

CREATE DATABASE database_name;
9. To create a table in Hive, you can use the following command:

CREATE TABLE table_name (
column1_name column1_data_type,
column2_name column2_data_type,
...
)
10.
In Hive, the DESCRIBE command is used to display metadata about a database or table. DESCRIBE shows the column names and data types for a table, while DESCRIBE EXTENDED shows additional information such as the table location and input format. DESCRIBE FORMATTED shows even more detailed information about a table, including its storage properties and SerDe (serializer/deserializer) information.


To skip header rows from a table in Hive, you can use the TBLPROPERTIES clause in your CREATE TABLE statement, like this:
sql
Copy code
CREATE TABLE mytable (
col1 STRING,
col2 INT,
col3 DOUBLE
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES ("skip.header.line.count"="1");
This will skip the first line of the input file when loading data into the table.
12.A Hive operator is a symbol or keyword used in HiveQL to perform various operations on data. There are several types of Hive operators, including:
Arithmetic operators (+, -, *, /, %)
Comparison operators (=, <>, <, <=, >, >=)
Logical operators (AND, OR, NOT)
Bitwise operators (&, |, ^, ~)
String operators (CONCAT, SUBSTR, LENGTH, etc.)
Aggregate functions (SUM, COUNT, AVG, etc.)
Window functions (ROW_NUMBER, RANK, DENSE_RANK, etc.)
13.Hive Built-In Functions are pre-defined functions that are included with Hive and can be used in HiveQL queries. These functions perform various operations on data, such as mathematical calculations, string manipulation, date/time manipulation, and type conversion. Some examples of Hive Built-In Functions include SUM, CONCAT, SUBSTR, FROM_UNIXTIME, and CAST.
14.Hive DDL (data definition language) commands are used to create, alter, and drop databases, tables, and other objects in Hive. Some common Hive DDL commands include CREATE DATABASE, CREATE TABLE, ALTER TABLE, and DROP TABLE.
Hive DML (data manipulation language) commands are used to insert, update, and delete data in Hive tables. Some common Hive DML commands include INSERT INTO, UPDATE, and DELETE.
15.SORT BY, ORDER BY, DISTRIBUTE BY, and CLUSTER BY are all used to sort data in Hive:
SORT BY sorts the data within each reducer based on the specified columns.
ORDER BY sorts the data globally based on the specified columns.
DISTRIBUTE BY determines which reducer will receive each row based on the specified columns.
CLUSTER BY is similar to DISTRIBUTE BY, but it also sorts the data within each reducer based on the specified columns.
16.Internal tables and external tables are two types of tables in Hive. Internal tables are managed by Hive, meaning that Hive manages the table metadata and the data stored in the table. External tables, on the other hand, are not managed by Hive and the data is stored outside of Hive's control.
When to choose internal table vs external table in Hive depends on the use case. If the data is generated and used exclusively within Hive, an internal table may be more appropriate. However, if the data is generated outside of Hive or needs to be shared with other systems, an external table may be more appropriate.
17.The data of a Hive table can be stored in various storage systems, such as HDFS, Amazon S3, or Microsoft Azure Data Lake Storage. The location of the data is specified in the table's LOCATION property.
18.Yes, it is possible to change the default location of a managed table in Hive. You can do this by using the ALTER TABLE command with the SET LOCATION clause, like this:
sql
Copy code
ALTER TABLE mytable SET LOCATION 'new_location';
This will change the location of the table's data to the specified directory.
19.The metastore in Hive is a repository that stores metadata about Hive tables, partitions, and databases. It includes information such as table schema, partitioning key, location of table data, and more. The metadata is used by Hive to optimize queries and manage table data. The default metastore database provided by Apache Hive is called Derby, which is an embedded Java-based database that is easy to set up and use for small to medium-sized deployments.
20.Hive does not store metadata information in HDFS because HDFS is a distributed file system designed for storing large amounts of data, but it is not optimized for storing and retrieving small pieces of metadata about the data. Storing metadata in HDFS would result in additional overhead and complexity for managing and querying the data. Instead, Hive uses a separate metastore repository to store the metadata, which can be backed by a relational database or other storage systems. This allows Hive to efficiently manage and query the metadata separately from the actual data stored in HDFS.


21.In Hive, a partition is a way of dividing a table into smaller, more manageable parts based on the values of one or more columns. Each partition is stored in a separate directory in HDFS with a specific path name that identifies the values of the partitioning columns.

Partitioning in Hive has several benefits, including:

Improved query performance: By partitioning a large table into smaller parts, Hive can eliminate the need to scan the entire table for queries that only require a subset of the data. This can significantly improve query performance, especially when dealing with large datasets.

Efficient data management: Partitioning can help to organize data in a more efficient way by allowing you to manage data at a finer level of granularity. For example, if you have a large dataset that is divided into time-based partitions, you can easily add, update, or delete data for specific time periods without affecting the rest of the data.

Easier data filtering: Partitioning makes it easier to filter data based on specific criteria, as Hive only needs to scan the relevant partitions rather than the entire table. This can simplify the development of complex queries and reduce the amount of data that needs to be transferred over the network.

Overall, partitioning is a powerful feature of Hive that can help to improve query performance, optimize data management, and simplify data filtering.


22.The main difference between dynamic partitioning and static partitioning in Hive is in how the partitions are created.
Static partitioning involves explicitly defining the partition columns and values while creating a table, and then loading data into specific partitions. This approach is suitable when the number of partitions is known in advance and doesn't change frequently.

Dynamic partitioning, on the other hand, involves creating partitions dynamically based on the data in a specific column. This is useful when the number of partitions is not known in advance or when new partitions need to be created frequently.
23.You can check if a particular partition exists in Hive using the SHOW PARTITIONS command. For example, to check if a partition exists for a specific date in a table called mytable, you can run the following command: SHOW PARTITIONS mytable PARTITION(dt='2023-04-28');
24.You can prevent a partition from being queried by setting the property hive.mapred.mode to nonstrict. This can be done by running the following command before running the query:

sql
Copy code
SET hive.mapred.mode=nonstrict;
This will prevent Hive from throwing an error if the partition is missing and simply skip it during the query execution.
25.Bucketing is a way of organizing data in Hive by dividing it into smaller, more manageable parts called buckets based on the values of a specific column. Buckets can help to improve query performance by reducing the amount of data that needs to be scanned for each query.
Hive distributes rows into buckets using a hashing algorithm based on the values of the bucketing column. Each bucket is assigned a unique bucket number based on the hash value of the bucketing column, and all rows with the same hash value are stored in the same bucket.
26.To enable bucketing in Hive, you need to specify the bucketing columns and the number of buckets when creating the table. This can be done using the CLUSTERED BY and SORTED BY clauses. For example, to create a table with two bucketing columns and 10 buckets, you can run the following command:
sql
Copy code
CREATE TABLE mytable (col1 INT, col2 STRING)
CLUSTERED BY (col1, col2) INTO 10 BUCKETS;
27.Bucketing helps in faster query execution by reducing the amount of data that needs to be scanned for each query. Since rows with the same hash value are stored in the same bucket, Hive can skip entire buckets during query execution if the query doesn't require data from that bucket. This can result in significant performance improvements for large datasets.
28.There are several ways to optimize Hive performance:

Partitioning: Partitioning tables can significantly improve query performance by reducing the amount of data that needs to be scanned for each query.
Bucketing: Bucketing can help to further optimize query performance by reducing the number of rows that need to be scanned within each partition.
Compression: Compressing data can help to reduce the amount of data that needs to be transferred over the network and stored on disk, which can improve query performance.
Vectorization: Vectorization is a technique that allows Hive to process data in batches instead of row by row, which can result in significant performance improvements for certain types of queries.
Caching: Caching frequently used data in memory can help to improve query performance by reducing the amount of time it takes to access the data from disk.
29.HCatalog is a tool that provides a unified interface for accessing data stored in Hadoop Distributed File System (HDFS) from different tools and applications. It allows you to access data stored in HDFS using a variety of programming languages and tools, including Pig, Hive, MapReduce, and HBase
30.Hive supports various types of joins, such as:
Inner Join: Returns only the matching records from both tables.
Left Join: Returns all records from the left table and the matching records from the right table.
Right Join: Returns all records from the right table and the matching records from the left table.
Full Outer Join: Returns all records from both tables.
Cartesian Join: Returns all possible combinations of records from both tables.
31.Yes, it is possible to create a Cartesian join between two tables in Hive by not specifying any join condition. However, it should be avoided as it can lead to huge amounts of data being processed and significant performance degradation.
32.Sort Merge Bucket (SMB) Join is a hybrid join strategy used in Hive for large tables with bucketed columns. In SMB Join, the join is performed on the buckets of both tables, which are then merged and sorted based on the join key. This method avoids shuffling the data across the cluster, leading to faster join performance.
33.The main difference between ORDER BY and SORT BY in Hive is that ORDER BY is used to sort the output of a query based on one or more columns in ascending or descending order, whereas SORT BY is used to sort the data before performing a reduce operation. If the data is already sorted and partitioned correctly, SORT BY is more efficient as it reduces the data shuffle phase.
34.The DISTRIBUTED BY clause in Hive is used to specify the columns on which data should be partitioned before sending to reducers. This clause ensures that records with the same values in the specified columns are processed by the same reducer, which reduces data movement and improves performance.
35.In Hive, data is transferred from HDFS to Hive through a process called MapReduce. Hive queries are first compiled into MapReduce jobs, which are then executed on the Hadoop cluster. During execution, the MapReduce jobs read data from HDFS and transfer it to the Hive query for processing.
36.Hive metastore stores metadata information about the tables, partitions, columns, and their associated properties. Whenever you run a Hive query, it creates a new metastore_db in the current working directory because the metastore needs a location to store its data files. If you want to use a specific location for the metastore_db, you can set the javax.jdo.option.ConnectionURL property to point to the desired location.


37.If the command "SET hive.enforce.bucketing=true;" is not issued before bucketing a table in Hive, then the table will still be bucketed but there will be no guarantee that the data is correctly partitioned in the buckets. This can lead to data skew and uneven distribution of data across buckets.
38.Yes, a table can be renamed in Hive using the ALTER TABLE RENAME command. For example, to rename a table named "old_table" to "new_table", the command would be:

ALTER TABLE old_table RENAME TO new_table;

To insert a new column named "new_col" of data type INT before an existing column named "x_col" in a Hive table named "my_table", the following ALTER TABLE command can be used:
ALTER TABLE my_table ADD COLUMNS (new_col INT) BEFORE x_col;

40.In Hive, SerDe (Serializer/Deserializer) is a way to serialize the data (convert from internal representation to a format that can be written to disk) and deserialize it (convert from disk format to internal representation). SerDe is used to map structured and semi-structured data in Hive tables to data stored in Hadoop Distributed File System (HDFS).
41.Hive uses SerDe to serialize and deserialize data in a table. When data is read from HDFS, it is deserialized into internal objects and when data is written to HDFS, it is serialized into a format that can be stored on disk. Hive supports many built-in SerDe classes, such as LazySimpleSerDe for plain text files, AvroSerDe for Avro data, and OrcSerDe for ORC files.
42.he name of the built-in SerDe in Hive is LazySimpleSerDe.
43.Custom SerDe is needed in Hive when the built-in SerDe classes do not fit the requirements of the data being used. Custom SerDe can be created to serialize and deserialize data using a specific format or data type.
44.Hive supports several complex data types, including arrays, maps, and structs. The syntax for declaring these types in Hive is as follows:

Arrays: ARRAY<element_type>
Maps: MAP<key_type, value_type>
Structs: STRUCT<field1: data_type1, field2: data_type2, ...>
45.Yes, hive queries can be executed from script files.

To execute Hive queries from a script file, we need to create a file containing the Hive queries that we want to execute and save it with a .hql extension. Then we can use the hive command with the -f option to run the script file.

For example, if we have a file named myqueries.hql containing our Hive queries, we can run the following command to execute the queries:

Copy code
hive -f myqueries.hql
This will execute all the queries in the myqueries.hql file in the order they are written.
46.The default record delimiter used for Hive text files is the newline character (\n) and the default field delimiter is the tab character (\t). However, we can customize these delimiters according to our data format by using the ROW FORMAT DELIMITED and FIELDS TERMINATED BY clauses in the CREATE TABLE statement.

47 To list all the databases in Hive whose name starts with s, we can use the following command:

sql
Copy code
SHOW DATABASES LIKE 's*';
This command will return the list of all databases whose name starts with the letter s.

48.The main difference between the LIKE and RLIKE operators in Hive is the type of regular expression pattern matching that they use.
LIKE operator uses simple regular expressions (with % and _ as wildcards) to match patterns in string values.
RLIKE operator, on the other hand, uses extended regular expressions (similar to POSIX regex) to match patterns in string values.
The LIKE operator is faster and easier to use for simple pattern matching tasks, while the RLIKE operator is more powerful and flexible, but also more complex to use.

49.To change the column data type in Hive, we can use the ALTER TABLE command. For example, to change the data type of the "age" column in the "employees" table to INTEGER, we can use the following command:

ALTER TABLE employees CHANGE age age INT;

50.We can use the CAST function to convert a string to a float value in Hive. For example, to convert the string "51.2" in the "salary" column to a float value, we can use the following command:

SELECT CAST(salary AS FLOAT) FROM employees;

When you cast 'abc' as INT in Hive, the result will be NULL because 'abc' is not a valid integer.
This query is inserting data from a source table ("staged_employees") into a destination table ("employees") while partitioning the data by two columns, "country" and "state". The query is also selecting certain columns from the source table, and adding the values of "cnty" and "st" as the partition keys for the destination table.
To overwrite data in a new table from the existing table, we can use the INSERT OVERWRITE TABLE command with a SELECT statement. For example, to create a new table "new_employees" with the same schema as "employees" and overwrite its data with the records where the "salary" column is greater than 5000, we can use the following command:
sql
Copy code
CREATE TABLE new_employees LIKE employees;

INSERT OVERWRITE TABLE new_employees
SELECT * FROM employees
WHERE salary > 5000;
54.The maximum size of a string data type supported by Hive is 2GB. Hive supports binary formats by using SerDe (Serializer/Deserializer) libraries that can read and write data in binary formats like Avro, SequenceFile, ORC, and Parquet.
55.Hive supports various file formats like TextFile, SequenceFile, RCFile, ORC, and Parquet. Hive can be used to process data stored in various file formats like CSV, JSON, and XML. Hive also supports integration with various external data sources like Apache HBase, Apache Cassandra, and Amazon S3.
56.ORC (Optimized Row Columnar) format tables help Hive to enhance its performance by storing data in a columnar format that improves query performance and reduces the storage space required. ORC files can be compressed and divided into smaller files, which makes it easier to read and process data in parallel.
57.Hive can avoid mapreduce while processing the query by using the Tez execution engine, which is an alternative to MapReduce. Tez is designed to execute complex DAGs (Directed Acyclic Graphs) of tasks and can perform better than MapReduce for certain types of queries.
58.In Hive, a view is a virtual table that is based on the result of a SELECT statement. Views are used to simplify complex queries and provide a consistent view of the data to the end-users. Indexing in Hive is a way to improve the performance of queries by creating indexes on one or more columns of a table. Indexes can be created using the CREATE INDEX statement.
59.Yes, the name of a view can be the same as the name of a Hive table. However, it is recommended to avoid using the same name to prevent confusion and ambiguity.
60.The costs associated with creating indexes on Hive tables include storage costs, computation costs, and maintenance costs. Storage costs are associated with the additional storage space required to store the index. Computation costs are associated with the additional CPU time required to update the index during data insertion or updates. Maintenance costs are associated with the additional effort required to manage and maintain the index.
61.To see the indexes on a table in Hive, you can use the following command:
graphql
Copy code
SHOW INDEXES ON table_name;
This will list all the indexes on the specified table.
62.To access subdirectories recursively in Hive queries, you can use the following syntax:
sql
Copy code
SELECT ...
FROM table_name
WHERE path LIKE 'path/to/subdirectory/%';
This will select all the rows in the specified subdirectory and any subdirectories that are nested within it. The '%' character is a wildcard that matches any string.
63.When you run a SELECT * query in Hive, it does not necessarily run MapReduce. If the table is stored in a file format that Hive can read natively, such as ORC or Parquet, it can simply read the data from the file without running MapReduce. However, if the table is stored in a text format, Hive will use MapReduce to read the data.
64.The EXPLODE function in Hive is used to transform a column that contains an array or a map into multiple rows. It generates a new row for each element in the array or map. This is useful when you want to perform calculations or analysis on individual elements in an array or map.
65.When you run Hive as a server, you can connect to it using the Thrift service. Thrift is a framework for building cross-language services. Hive exposes a Thrift service that allows applications to connect to it and execute HiveQL queries. Applications can use any programming language that has a Thrift client library to connect to Hive.
66.Yes, the default location of a managed table can be changed in Hive. This can be done by setting the property hive.metastore.warehouse.dir in the hive-site.xml configuration file to the desired location. The new location must be accessible by the Hive service user and have the required read and write permissions.
67.The Hive ObjectInspector is a framework that provides a set of APIs for inspecting the structure and data of objects in Hive. It is used to read data from and write data to Hive tables. The ObjectInspector defines the type and structure of data stored in Hive tables, and provides methods to read and write data in a specific format. It is used to convert data between the internal representation used by Hive and external data formats, such as text or binary formats.
68.UDF stands for User-Defined Functions. UDFs are custom functions that can be written in Java or any other supported programming language to extend the functionality of Hive. UDFs can be used to perform complex calculations, data transformations, and other custom operations on data stored in Hive tables. UDFs can be used in Hive queries just like built-in functions, allowing users to perform custom operations on data with ease.
69.To extract data from HDFS to Hive, we can use the LOAD DATA command in Hive. The syntax for this command is:

php
Copy code
LOAD DATA INPATH '<HDFS_PATH>' [OVERWRITE] INTO TABLE <table_name> [PARTITION (<partition_column>=, ...)];
Where <HDFS_PATH> is the location of the data in HDFS, <table_name> is the name of the Hive table where the data will be loaded, and <partition_column> and are optional partition columns and values.
70.TextInputFormat and SequenceFileInputFormat are two file input formats supported by Hive. TextInputFormat is the default input format for text files in Hive. It reads text files line by line and each line is considered a separate record. SequenceFileInputFormat is used for reading binary key-value pairs in sequence file format. It can be used to read data that has been compressed and/or serialized.


71.There are several ways to prevent a large job from running for a long time in Hive:
Limit the data processed: If possible, limit the data processed by your query by using filters or partitioning. This will reduce the amount of data processed and the time taken for the job to complete.
Optimize the query: Optimize your Hive query by ensuring that it is written efficiently and making use of Hive's optimization features such as the Tez execution engine.
Increase resources: Increase the resources available to your Hive job, such as increasing the number of nodes in the cluster, increasing the memory allocated to each node, or increasing the number of tasks that can run in parallel.
Use compression: Compress the input data to reduce the amount of data processed and the time taken for the job to complete.
Use bucketing: Bucketing can improve the performance of certain queries by reducing the amount of data that needs to be processed.
72.The explode() function in Hive is used to transform a collection column into multiple rows, one for each element in the collection. It is typically used when a table contains columns with collection types, such as arrays or maps, and you want to query or manipulate the individual elements of the collection. The explode() function generates a new row for each element in the collection, with the rest of the columns in the row repeated for each element. For example, if you have a table with a column containing an array of strings, you can use the explode() function to generate a new row for each string in the array.
73.Hive can process various types of data formats, including structured, semi-structured, and unstructured data. It supports the following file formats:

Text files: Plain text files in which each line represents a record.
Sequence files: Binary file format optimized for Hadoop processing.
ORC files: Optimized Row Columnar (ORC) file format, designed for high performance and low storage overhead.
Parquet files: Columnar file format designed for efficient storage and processing of large datasets.
AVRO files: Compact binary file format designed for efficient serialization and deserialization of structured data.
RC files: Columnar file format optimized for Hadoop processing.
In addition, Hive can also access data stored in HBase tables, and can integrate with other data processing systems such as Pig and Spark. Hive is designed to be flexible and extensible, and can be extended to support new data formats and data processing systems through custom InputFormat, OutputFormat, and SerDe classes.
74.Whenever we run a Hive query, a new metastore_db is created. Why?
The metastore_db is a relational database that stores metadata about the Hive tables, partitions, columns, and their properties. Whenever a Hive query is executed, it needs to access the metadata stored in the metastore_db. To enable this, a connection needs to be established between the Hive query engine and the metastore_db.

By default, the metastore_db is located in the local file system of the Hive server. When a new query is executed, a new connection needs to be established, and if the metadata is not available in the metastore_db, a new one is created. This ensures that the query has access to the metadata required for execution.

75.Can we change the data type of a column in a hive table? Write a complete query.
Yes, we can change the data type of a column in a Hive table using the ALTER TABLE command. The syntax of the command is as follows:

sql
Copy code
ALTER TABLE table_name CHANGE COLUMN column_name new_column_name new_data_type;
For example, to change the data type of a column named "age" from INT to STRING in a table named "employees", the command would be:

sql
Copy code
ALTER TABLE employees CHANGE COLUMN age age_str STRING;
76.While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file?
When loading data into a Hive table using the LOAD DATA command, we need to specify the location of the input data file. To specify a file located in the Hadoop Distributed File System (HDFS), we need to specify the complete HDFS path of the file.

For example, to load data from a file named "input_file.csv" located in the "/user/hive/data" directory of the HDFS, the command would be:

sql
Copy code
LOAD DATA INPATH '/user/hive/data/input_file.csv' INTO TABLE table_name;
Here, the keyword "INPATH" is used to specify the HDFS path of the input file.
77.What is the precedence order in Hive configuration?
The precedence order in Hive configuration refers to the order in which different configuration properties are applied when there is a conflict or ambiguity. The order of precedence is as follows:

Configuration set using Hadoop configuration files such as core-site.xml, hdfs-site.xml, etc.
Configuration set using the hive-site.xml file in the Hive installation directory.
Configuration set using the SET command in the Hive shell or in the Hive queries.
Default configuration set in the Hive source code.
This means that the configuration properties set using Hadoop configuration files take the highest precedence, followed by those set using the hive-site.xml file, then those set using the SET command, and finally the default configuration properties set in the Hive source code.

78.Which interface is used for accessing the Hive metastore?
The Hive Metastore is accessed using the Thrift interface. The Thrift interface provides a generic mechanism for calling remote procedures using a client-server architecture. In the case of Hive, the Thrift interface is used to connect the Hive Query Engine with the Hive Metastore Server.

The Thrift interface allows clients to connect to the Hive Metastore Server and access the metadata stored in the metastore_db. The metadata includes information about the tables, partitions, columns, and their properties, as well as the location of the data in HDFS.

79.Yes, it is possible to compress JSON in the Hive external table. Hive supports various file formats that support compression, such as SequenceFile and ORC, which can be used to store JSON data.
80.Local metastores are used for development and testing purposes, where the metadata is stored in a local file system. Remote metastores are used for production environments, where the metadata is stored in a separate database server to provide better performance and scalability.
81.Archiving tables in Hive involves moving the data from an existing table to an archived table, which can be used for backup, data retention, or analysis purposes. Archiving helps in reducing the size of the original table and improving the performance of queries.
82.DBPROPERTY is a function in Hive that is used to retrieve the value of a property for a specified database. It is used to get the value of various database properties, such as the database name, location, owner, and the number of tables.
83.Local mode and MapReduce mode are two execution modes supported by Hive. In local mode, Hive runs the query on the local machine, while in MapReduce mode, it runs the query on a Hadoop cluster using MapReduce jobs. Local mode is generally used for small datasets or for testing purposes, while MapReduce mode is used for large datasets in production environments. MapReduce mode provides better performance and scalability, but it requires a Hadoop cluster to be set up.